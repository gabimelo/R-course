names(df)[names(df) == 'X1'] <- 'spam'
df$spam = factor(df$spam)
# df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
misclassification
library(rpart)
df <- read.csv("adult/df.csv")
test_set <- read.csv("adult/test_set.csv")
df['X'] <- NULL
test_set['X'] <- NULL
df$over50k = factor(df$over50k)
test_set$over50k = factor(test_set$over50k)
classifier = rpart(formula = over50k~., data = df, control=rpart.control(minsplit=15, minbucket=5,cp=0.005))
y_pred = predict(classifier, newdata = test_set[-6], type = 'class')
cm = table(test_set[,6], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
accuracy = (TN + TP)/total # 0.7629
sensitivity = TP/(TP+FN) # 0.6435
specificity = TN/(TN+FP) # 0.8008
precision = TP/(TP+FP) # 0.5060
misclassification = (FN+FP)/total # 0.1848
ROC = sensitivity * (1 - specificity) # 0.1282
library(Metrics)
# test_set[,6] = factor(test_set[,6])
# y_pred = factor(y_pred)
mse(actual = as.integer(test_set[,6]), predicted = as.integer(y_pred))
# 0.2371
plot(classifier)
text(classifier)
misclassification
library(e1071)
df <- read.csv("adult/df.csv")
test_set <- read.csv("adult/test_set.csv")
df['X'] <- NULL
test_set['X'] <- NULL
df$over50k = factor(df$over50k)
test_set$over50k = factor(test_set$over50k)
classifier = naiveBayes(x = df[-6], y = df$over50k)
y_pred = predict(classifier, newdata = test_set[-6])
cm = table(test_set[,6], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
accuracy = (TN + TP)/total # 0.7578
sensitivity = TP/(TP+FN) # 0.6646
specificity = TN/(TN+FP) # 0.7867
precision = TP/(TP+FP) # 0.4907
misclassification = (FN+FP)/total # 0.1848
ROC = sensitivity * (1 - specificity) # 0.1417
library(Metrics)
mse(actual = as.integer(test_set[,6]), predicted = as.integer(y_pred))
# 0.2421
misclassification
library(bnlearn)
df <- read.csv("adult/df.csv")
test_set <- read.csv("adult/test_set.csv")
df['X'] <- NULL
test_set['X'] <- NULL
df[] <- lapply(df, factor)
test_set[] <- lapply(test_set, factor)
# manual graph, learned probabilities
dag = model2network("[american][white|american][male][married][primeage][over50k|white:male:married:primeage]")
manual_net_learned_probs <- bn.fit(dag, df)
plot(dag)
# manual probabilities and graph:
cptA = matrix(c(0.1, 0.9), ncol = 2, dimnames = list(NULL, c(0, 1)))
cptMR = matrix(c(0.55, 0.45), ncol = 2, dimnames = list(NULL, c(0, 1)))
cptML = matrix(c(0.33, 0.67), ncol = 2, dimnames = list(NULL, c(0, 1)))
cptP = matrix(c(0.36, 0.64), ncol = 2, dimnames = list(NULL, c(0, 1)))
cptW = c(0.35, 0.65, 0.12, 0.88)
dim(cptW) = c(2, 2)
dimnames(cptW) = list("white" = c(0, 1), "american" =  c(0, 1))
# newdata <- subset(df, !male & married & !primeage & white, select=c(over50k))
# table(newdata)
# sum = sum + count(newdata)
cptO = c(0.99, 0.01, 0.98, 0.02, 0.96, 0.04, 0.97, 0.03,
0.7, 0.3, 0.67, 0.33, 0.8, 0.2, 0.75, 0.25,
0.95, 0.05, 0.92, 0.08, 0.91, 0.09, 0.83, 0.17,
0.62, 0.38, 0.44, 0.56, 0.56, 0.44, 0.50, 0.50)
dim(cptO) = c(2, 2, 2, 2, 2)
dimnames(cptO) = list("over50k" = c(0, 1), "white" =  c(0, 1), "male" =  c(0, 1), "married" =  c(0, 1), "primeage" =  c(0, 1))
net = model2network("[american][white|american][male][married][primeage][over50k|white:male:married:primeage]")
manual = custom.fit(net, dist = list(american = cptA, white = cptW, male = cptML, married = cptMR, primeage = cptP, over50k = cptO))
manual
plot(net)
# learned probabilities and graph:
pdag = iamb(df)
pdag
plot(pdag)
# dag2 = set.arc(dag2, from = "male", to = "married")
# dag2 = set.arc(dag2, from = "male", to = "over50k")
pdag = set.arc(pdag, from = "married", to = "male")
pdag = set.arc(pdag, from = "primeage", to = "married")
plot(pdag)
automatic = bn.fit(pdag, df)
# scoring
score(dag, df)
score(net, df)
score(pdag, df)
# testing the predictions:
y_pred = predict(manual_net_learned_probs, node='over50k', data=test_set[-6])
y_pred = predict(manual, node='over50k', data=test_set[-6])
y_pred = predict(automatic, node='over50k', data=test_set[-6])
cm = table(test_set[,6], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
accuracy = (TN + TP)/total # 0.7661; 0.7676; 0.7688
sensitivity = TP/(TP+FN) # 0.0702; 0.4001; 0.6578
specificity = TN/(TN+FP) # 0.9814; 0.8812; 0.8031
precision = TP/(TP+FP) # 0.5389; 0.5103; 0.5082
ROC = sensitivity * (1 - specificity) # 0.0013; 0.0475; 0.1295
library(Metrics)
mse(actual = as.integer(test_set[,6]), predicted = as.integer(y_pred))
# 0.2338; 0.2324; 0.2312
misclassification = (FN+FP)/total # 0.1848
misclassification
y_pred = predict(manual, node='over50k', data=test_set[-6])
cm = table(test_set[,6], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
accuracy = (TN + TP)/total # 0.7661; 0.7676; 0.7688
sensitivity = TP/(TP+FN) # 0.0702; 0.4001; 0.6578
specificity = TN/(TN+FP) # 0.9814; 0.8812; 0.8031
precision = TP/(TP+FP) # 0.5389; 0.5103; 0.5082
misclassification = (FN+FP)/total # ; ; 0.2312
ROC = sensitivity * (1 - specificity) # 0.0013; 0.0475; 0.1295
library(Metrics)
mse(actual = as.integer(test_set[,6]), predicted = as.integer(y_pred))
# 0.2338; 0.2324; 0.2312
misclassification
y_pred = predict(manual_net_learned_probs, node='over50k', data=test_set[-6])
cm = table(test_set[,6], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
accuracy = (TN + TP)/total # 0.7661; 0.7676; 0.7688
sensitivity = TP/(TP+FN) # 0.0702; 0.4001; 0.6578
specificity = TN/(TN+FP) # 0.9814; 0.8812; 0.8031
precision = TP/(TP+FP) # 0.5389; 0.5103; 0.5082
misclassification = (FN+FP)/total # ; 0.2383; 0.2312
ROC = sensitivity * (1 - specificity) # 0.0013; 0.0475; 0.1295
library(Metrics)
mse(actual = as.integer(test_set[,6]), predicted = as.integer(y_pred))
# 0.2338; 0.2324; 0.2312
misclassification
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
df$spam = factor(df$spam)
# df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
mean(df)
df = read.csv('spambase/spambase.data')
mean(df)
means <- lapply(df, mean)
means
head(df)
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naive.bayes(x = training_set[-58], training = "spam")
# classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naive.bayes(x = training_set, training = "spam")
# classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
# classifier = naive.bayes(x = training_set, training = "spam")
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
means <- sapply(df, mean)
df = read.csv('spambase/spambase.data')
means <- sapply(df, mean)
menas
means
df <- (df > means)
df = read.csv('spambase/spambase.data')
df[] <- (df[] > means)
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
df[] <- 1*(df[] > means)
df = read.csv('spambase/spambase.data')
means <- sapply(df, mean)
df[] <- 1*(df[] > means)
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
