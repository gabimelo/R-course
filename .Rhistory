precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
df[] <- 1*(df[] > means)
df = read.csv('spambase/spambase.data')
means <- sapply(df, mean)
df[] <- 1*(df[] > means)
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
# last column (index 58): 1 spam, 0 not
df = read.csv('spambase/spambase.data')
names <- colnames(df)[0:57]
for (name in names){
mean <- mean(df[[name]])
df[[name]][df[[name]] <= mean] <- 0
df[[name]][df[[name]] > mean] <- 1
}
names(df)[names(df) == 'X1'] <- 'spam'
# df$spam = factor(df$spam)
df[] <- lapply(df, factor)
library(caTools)
split = sample.split(df$spam, SplitRatio = 0.6)
training_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)
library(e1071)
classifier = naiveBayes(x = training_set[-58], y = training_set$spam)
y_pred <- predict(classifier, test_set)
cm = table(test_set[,"spam"], y_pred)
TN = true_negatives = cm[1, "0"]
FN = false_negatives = cm[2, "0"]
FP = false_positives = cm[1, "1"]
TP = true_positives = cm[2, "1"]
total = TN + FN + FP + TP
# é muito ruim ter o número de false positives tão elevado
accuracy = (TN + TP)/total # 0.8054 ---> se coloco como factor todas as colunas, aqui aumenta pra 90
sensitivity = TP/(TP+FN) # 0.96 --> 83
specificity = TN/(TN+FP) # 0.7049 --> 95
precision = TP/(TP+FP) # 0.6790 --> 91
misclassification = (FN+FP)/total # 0.1848 --> 0.09
ROC = sensitivity * (1 - specificity) # 0.2833 --> 0.04
library(Metrics)
test_set[,58] = factor(test_set[,58], levels = c(0,1))
y_pred = factor(y_pred, levels = c(0,1))
mse(actual = as.integer(test_set[,58]), predicted = as.integer(y_pred))
# 0.1945 ---> 0.097
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(caret)
# DATA LOAD AND PREPROCESSING
cnames <- c("age", "workclass", "fnlwgt", "education", "education.num", "marital.status",
"occupation", "relationship", "race", "sex", "capital.gain", "capital.loss",
"hours.per.week", "native.country", "income")
df <- read.csv("adult/adult.csv", col.names = cnames, header = FALSE)
test_set <- read.csv("adult/adult_test.csv", col.names = cnames, header = FALSE)
# columns with missing data: workclass, occupation, native country
df <- subset(df,!(df$workclass==" ?" | df$occupation==" ?" | df$native.country==" ?"))
# df now has length 30162 (original 32561)
test_set <- subset(test_set,!(test_set$workclass==" ?" | test_set$occupation==" ?" | test_set$native.country==" ?"))
# df now has length 15060 (original = 16281)
prop.table(table(df$income))
prop.table(table(test_set$income))
# pretty much same distribution between classes
# there's a bit of class imbalance
# duplicate column:
df$education <- NULL
test_set$education <- NULL
# DECISION TREE
classifier = rpart(formula = income~., data = df, method = "class",
control=rpart.control(cp=0.0035))
y_pred = predict(classifier, newdata = test_set[-14], type = 'class')
#printcp(classifier)
#plotcp(classifier)
#summary(classifier)
fancyRpartPlot(classifier)
confusionMatrix(y_pred, test_set$income)
# cp 0.0030 acc 0.8541169 terminal nodes 17
# cp 0.0035 acc 0.847676 terminal nodes 11
# cp 0.0045 acc 0.8466135 terminal nodes 10
# LOGISTIC REGRESSION
df$capital.gain.greater.mean = factor(ifelse(df$capital.gain > mean(df$capital.gain), 1, 0))
df$capital.loss.greater.mean = factor(ifelse(df$capital.loss > mean(df$capital.loss), 1, 0))
df$capital.gain <- NULL
df$capital.loss <- NULL
test_set$capital.gain.greater.mean = factor(ifelse(test_set$capital.gain > mean(test_set$capital.gain), 1, 0))
test_set$capital.loss.greater.mean = factor(ifelse(test_set$capital.loss > mean(test_set$capital.loss), 1, 0))
test_set$capital.gain <- NULL
test_set$capital.loss <- NULL
classifier = glm(formula = income ~ ., family = binomial, data = df)
prob_pred = predict(classifier, newdata = test_set[-12], type = 'response')
y_pred = ifelse(prob_pred > 0.5, 1, 0)
confusionMatrix(y_pred, test_set$income)
# 0.8403054 with capital gain and loss as factors representing if greater than mean
# 0.8292165 without capital gain and loss altogether
## CROSS VALIDATION
set.seed(87)
df <- read.csv("adult/adult.csv", col.names = cnames, header = FALSE)
test_set <- read.csv("adult/adult_test.csv", col.names = cnames, header = FALSE)
test_set$income <- sapply(test_set$income, as.character)
test_set$income[test_set$income == ' <=50K.'] <- ' <=50K'
test_set$income[test_set$income == ' >50K.'] <- ' >50K'
test_set$income = factor(test_set$income)
#df <- rbind(df, test_set)
df <- subset(df,!(df$workclass==" ?" | df$occupation==" ?" | df$native.country==" ?"))
test_set <- subset(test_set,!(test_set$workclass==" ?" | test_set$occupation==" ?" | test_set$native.country==" ?"))
df$education <- NULL
test_set$education <- NULL
# DECISION TREE
classifier = train(income ~ ., df, method="rpart",
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred = predict(classifier, newdata = test_set[-14], type = 'raw')
fancyRpartPlot(classifier$finalModel)
confusionMatrix(y_pred, test_set$income)
# cp 0.0369 acc 0.8541169 terminal nodes 4
# LOGISTIC REGRESSION
df$capital.gain.greater.mean = factor(ifelse(df$capital.gain > mean(df$capital.gain), 1, 0))
df$capital.loss.greater.mean = factor(ifelse(df$capital.loss > mean(df$capital.loss), 1, 0))
df$capital.gain <- NULL
df$capital.loss <- NULL
test_set$capital.gain.greater.mean = factor(ifelse(test_set$capital.gain > mean(test_set$capital.gain), 1, 0))
test_set$capital.loss.greater.mean = factor(ifelse(test_set$capital.loss > mean(test_set$capital.loss), 1, 0))
test_set$capital.gain <- NULL
test_set$capital.loss <- NULL
classifier = train(income ~ ., method='glm', family = binomial, data = df,
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred = predict(classifier, newdata = test_set[-12], type = 'raw')
confusionMatrix(y_pred, test_set$income)
# 0.8408933 on classifier
# 0.8402 on test set predictions
classifier
cv_dt = train(income ~ ., df, method="rpart",
control = rpart.control(cp=0.003),
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred = predict(cv_dt, newdata = test_set[-14], type = 'raw')
fancyRpartPlot(cv_dt$finalModel)
confusionMatrix(y_pred, test_set$income)
cv_dt
# DECISION TREE
cv_dt = train(income ~ ., df, method="rpart",
control = rpart.control(cp=0.004),
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
cv_dt
y_pred = predict(cv_dt, newdata = test_set[-14], type = 'raw')
str(test_set)
str(df)
y_pred = predict(cv_dt, newdata = test_set[-14], type = 'raw')
fancyRpartPlot(cv_dt$finalModel)
t.test(income ~ ., data = df)
t.test(income ~ ., data = test_set)
cv_dt
cv_lr
df <- read.csv("adult/adult.csv", col.names = cnames, header = FALSE)
test_set <- read.csv("adult/adult_test.csv", col.names = cnames, header = FALSE)
test_set$income <- sapply(test_set$income, as.character)
test_set$income[test_set$income == ' <=50K.'] <- ' <=50K'
test_set$income[test_set$income == ' >50K.'] <- ' >50K'
test_set$income = factor(test_set$income)
#df <- rbind(df, test_set)
df <- subset(df,!(df$workclass==" ?" | df$occupation==" ?" | df$native.country==" ?"))
test_set <- subset(test_set,!(test_set$workclass==" ?" | test_set$occupation==" ?" | test_set$native.country==" ?"))
df$education <- NULL
test_set$education <- NULL
# DECISION TREE
cv_dt = train(income ~ ., df, method="rpart",
control = rpart.control(cp=0.004),
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred = predict(cv_dt, newdata = test_set[-14], type = 'raw')
fancyRpartPlot(cv_dt$finalModel)
confusionMatrix(y_pred, test_set$income)
# cp 0.003 acc 0.8402 terminal nodes 6
# 0.00792488  0.8231213 on classifier
# LOGISTIC REGRESSION
df$capital.gain.greater.mean = factor(ifelse(df$capital.gain > mean(df$capital.gain), 1, 0))
df$capital.loss.greater.mean = factor(ifelse(df$capital.loss > mean(df$capital.loss), 1, 0))
df$capital.gain <- NULL
df$capital.loss <- NULL
test_set$capital.gain.greater.mean = factor(ifelse(test_set$capital.gain > mean(test_set$capital.gain), 1, 0))
test_set$capital.loss.greater.mean = factor(ifelse(test_set$capital.loss > mean(test_set$capital.loss), 1, 0))
test_set$capital.gain <- NULL
test_set$capital.loss <- NULL
cv_lr = train(income ~ ., method='glm', family = binomial, data = df,
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred = predict(cv_lr, newdata = test_set[-12], type = 'raw')
confusionMatrix(y_pred, test_set$income)
cv_dt
cv_lr
?resamples
resamples(test_set, 10)
resamples(test_set)
resamples(cv_lr)
?summary.resamples
results <- resamples(list(DT=cv_dt, LR=cv_lr))
summary(results)
plot(results)
results
results.x
summary.diff(results)
summary(diff(results))
results$values
results$values$`DT~Accuracy`
results$values$`LR~Accuracy`
DT = results$values$`DT~Accuracy`
LR = results$values$`LR~Accuracy`
cv_lr
cv_dt
t.test(DT,LR,paired=TRUE,alternative="greater")
t.test(DT,LR,paired=FALSE,alternative="greater")
t.test(DT,LR,paired=TRUE,alternative="greater")
t.test(DT,LR,alternative="greater")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(caret)
# DATA LOAD AND PREPROCESSING
cnames <- c("age", "workclass", "fnlwgt", "education", "education.num", "marital.status",
"occupation", "relationship", "race", "sex", "capital.gain", "capital.loss",
"hours.per.week", "native.country", "income")
df <- read.csv("adult/adult.csv", col.names = cnames, header = FALSE)
test_set <- read.csv("adult/adult_test.csv", col.names = cnames, header = FALSE)
test_set$income <- sapply(test_set$income, as.character)
test_set$income[test_set$income == ' <=50K.'] <- ' <=50K'
test_set$income[test_set$income == ' >50K.'] <- ' >50K'
test_set$income = factor(test_set$income)
# columns with missing data: workclass, occupation, native country
df <- subset(df,!(df$workclass==" ?" | df$occupation==" ?" | df$native.country==" ?"))
# df now has length 30162 (original 32561)
test_set <- subset(test_set,!(test_set$workclass==" ?" | test_set$occupation==" ?" | test_set$native.country==" ?"))
# df now has length 15060 (original = 16281)
prop.table(table(df$income))
prop.table(table(test_set$income))
# pretty much same distribution between classes
# there's a bit of class imbalance
# duplicate column:
df$education <- NULL
test_set$education <- NULL
# factorize capital gain and loss (LR can't deal with them numerically)
df$capital.gain.greater.mean = factor(ifelse(df$capital.gain > mean(df$capital.gain), 1, 0))
df$capital.loss.greater.mean = factor(ifelse(df$capital.loss > mean(df$capital.loss), 1, 0))
df$capital.gain <- NULL
df$capital.loss <- NULL
test_set$capital.gain.greater.mean = factor(ifelse(test_set$capital.gain > mean(test_set$capital.gain), 1, 0))
test_set$capital.loss.greater.mean = factor(ifelse(test_set$capital.loss > mean(test_set$capital.loss), 1, 0))
test_set$capital.gain <- NULL
test_set$capital.loss <- NULL
# DECISION TREE
dt = rpart(formula = income~., data = df, method = "class",
control=rpart.control(cp=0.0035))
y_pred = predict(dt, newdata = test_set[-14], type = 'class')
#printcp(dt)
#plotcp(dt)
#summary(dt)
fancyRpartPlot(dt)
confusionMatrix(y_pred, test_set$income)
# cp 0.0030 acc 0.8541169 terminal nodes 17
# cp 0.0035 acc 0.847676 terminal nodes 11
# cp 0.0045 acc 0.8466135 terminal nodes 10
# LOGISTIC REGRESSION
lr = glm(formula = income ~ ., family = binomial, data = df)
prob_pred = predict(lr, newdata = test_set[-12], type = 'response')
y_pred = ifelse(prob_pred > 0.5, 1, 0)
confusionMatrix(y_pred, test_set$income)
# 0.8403054 with capital gain and loss as factors representing if greater than mean
# 0.8292165 without capital gain and loss altogether
## CROSS VALIDATION
set.seed(87)
# DECISION TREE
cv_dt = train(income ~ ., df, method="rpart",
control = rpart.control(cp=0.004),
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred_cv_dt = predict(cv_dt, newdata = test_set[-14], type = 'raw')
fancyRpartPlot(cv_dt$finalModel)
confusionMatrix(y_pred_cv_dt, test_set$income)
# cp 0.003 acc 0.8402 terminal nodes 6
# 0.00792488  0.8231213 on classifier
# cp 0.004 acc 0.8299  terminal nodes 4
# 0.03689398  0.8349904 on classifier
# LOGISTIC REGRESSION
cv_lr = train(income ~ ., method='glm', family = binomial, data = df,
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred_cv_lr = predict(cv_lr, newdata = test_set[-12], type = 'raw')
confusionMatrix(y_pred_cv_lr, test_set$income)
# 0.8407271 on classifier
# 0.8402 on test set predictions
# T-TEST
t.test(DT,LR,paired=TRUE,alternative="greater")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(caret)
set.seed(87)
# DATA LOAD AND PREPROCESSING
cnames <- c("age", "workclass", "fnlwgt", "education", "education.num", "marital.status",
"occupation", "relationship", "race", "sex", "capital.gain", "capital.loss",
"hours.per.week", "native.country", "income")
df <- read.csv("adult/adult.csv", col.names = cnames, header = FALSE)
test_set <- read.csv("adult/adult_test.csv", col.names = cnames, header = FALSE)
test_set$income <- sapply(test_set$income, as.character)
test_set$income[test_set$income == ' <=50K.'] <- ' <=50K'
test_set$income[test_set$income == ' >50K.'] <- ' >50K'
test_set$income = factor(test_set$income)
# columns with missing data: workclass, occupation, native country
df <- subset(df,!(df$workclass==" ?" | df$occupation==" ?" | df$native.country==" ?"))
# df now has length 30162 (original 32561)
test_set <- subset(test_set,!(test_set$workclass==" ?" | test_set$occupation==" ?" | test_set$native.country==" ?"))
# df now has length 15060 (original = 16281)
prop.table(table(df$income))
prop.table(table(test_set$income))
# pretty much same distribution between classes
# there's a bit of class imbalance
# duplicate column:
df$education <- NULL
test_set$education <- NULL
# factorize capital gain and loss (LR can't deal with them numerically)
df$capital.gain.greater.mean = factor(ifelse(df$capital.gain > mean(df$capital.gain), 1, 0))
df$capital.loss.greater.mean = factor(ifelse(df$capital.loss > mean(df$capital.loss), 1, 0))
df$capital.gain <- NULL
df$capital.loss <- NULL
test_set$capital.gain.greater.mean = factor(ifelse(test_set$capital.gain > mean(test_set$capital.gain), 1, 0))
test_set$capital.loss.greater.mean = factor(ifelse(test_set$capital.loss > mean(test_set$capital.loss), 1, 0))
test_set$capital.gain <- NULL
test_set$capital.loss <- NULL
# DECISION TREE
dt = rpart(formula = income~., data = df, method = "class",
control=rpart.control(cp=0.0035))
y_pred_dt = predict(dt, newdata = test_set[-12], type = 'class')
#printcp(dt)
#plotcp(dt)
#summary(dt)
fancyRpartPlot(dt)
confusionMatrix(y_pred_dt, test_set$income)
# cp 0.0030 acc 0.8541169 terminal nodes 17
# cp 0.0035 acc 0.847676 terminal nodes 11
# cp 0.0045 acc 0.8466135 terminal nodes 10
# LOGISTIC REGRESSION
lr = glm(formula = income ~ ., family = binomial, data = df)
prob_pred = predict(lr, newdata = test_set[-12], type = 'response')
y_pred_lr = ifelse(prob_pred > 0.5, 1, 0)
confusionMatrix(y_pred_lr, test_set$income)
# 0.8403054 with capital gain and loss as factors representing if greater than mean
# 0.8292165 without capital gain and loss altogether
## CROSS VALIDATION
# DECISION TREE
cv_dt = train(income ~ ., df, method="rpart",
control = rpart.control(cp=0.004),
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred_cv_dt = predict(cv_dt, newdata = test_set[-12], type = 'raw')
fancyRpartPlot(cv_dt$finalModel)
confusionMatrix(y_pred_cv_dt, test_set$income)
# cp 0.003 acc 0.8402 terminal nodes 6
# 0.00792488  0.8231213 on classifier
# cp 0.004 acc 0.8299  terminal nodes 4
# 0.03689398  0.8349904 on classifier
# LOGISTIC REGRESSION
cv_lr = train(income ~ ., method='glm', family = binomial, data = df,
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred_cv_lr = predict(cv_lr, newdata = test_set[-12], type = 'raw')
confusionMatrix(y_pred_cv_lr, test_set$income)
# 0.8407271 on classifier
# 0.8402 on test set predictions
# T-TEST
t.test(DT,LR,paired=TRUE,alternative="greater")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(caret)
set.seed(87)
# DATA LOAD AND PREPROCESSING
cnames <- c("age", "workclass", "fnlwgt", "education", "education.num", "marital.status",
"occupation", "relationship", "race", "sex", "capital.gain", "capital.loss",
"hours.per.week", "native.country", "income")
df <- read.csv("adult/adult.csv", col.names = cnames, header = FALSE)
test_set <- read.csv("adult/adult_test.csv", col.names = cnames, header = FALSE)
test_set$income <- sapply(test_set$income, as.character)
test_set$income[test_set$income == ' <=50K.'] <- ' <=50K'
test_set$income[test_set$income == ' >50K.'] <- ' >50K'
test_set$income = factor(test_set$income)
# columns with missing data: workclass, occupation, native country
df <- subset(df,!(df$workclass==" ?" | df$occupation==" ?" | df$native.country==" ?"))
# df now has length 30162 (original 32561)
test_set <- subset(test_set,!(test_set$workclass==" ?" | test_set$occupation==" ?" | test_set$native.country==" ?"))
prop.table(table(df$income))
prop.table(table(test_set$income))
# duplicate column:
df$education <- NULL
test_set$education <- NULL
# factorize capital gain and loss (LR can't deal with them numerically)
df$capital.gain.greater.mean = factor(ifelse(df$capital.gain > mean(df$capital.gain), 1, 0))
df$capital.loss.greater.mean = factor(ifelse(df$capital.loss > mean(df$capital.loss), 1, 0))
df$capital.gain <- NULL
df$capital.loss <- NULL
test_set$capital.gain.greater.mean = factor(ifelse(test_set$capital.gain > mean(test_set$capital.gain), 1, 0))
test_set$capital.loss.greater.mean = factor(ifelse(test_set$capital.loss > mean(test_set$capital.loss), 1, 0))
test_set$capital.gain <- NULL
test_set$capital.loss <- NULL
# DECISION TREE
dt = rpart(formula = income~., data = df, method = "class",
control=rpart.control(cp=0.0035))
y_pred_dt = predict(dt, newdata = test_set[-12], type = 'class')
#printcp(dt)
#plotcp(dt)
#summary(dt)
fancyRpartPlot(dt)
confusionMatrix(y_pred_dt, test_set$income)
# LOGISTIC REGRESSION
lr = glm(formula = income ~ ., family = binomial, data = df)
prob_pred = predict(lr, newdata = test_set[-12], type = 'response')
y_pred_lr = ifelse(prob_pred > 0.5, 1, 0)
confusionMatrix(y_pred_lr, test_set$income)
y_pred_lr
prob_pred
prob_pred = predict(lr, newdata = test_set[-12], type = 'class')
prob_pred = predict(lr, newdata = test_set[-12], type = 'raw')
response
prob_pred = predict(lr, newdata = test_set[-12], type = 'response')
str(test_set[-12])
str(test_set[,12])
y_pred_lr = ifelse(prob_pred > 0.5, ' >50K', ' <=50K')
confusionMatrix(y_pred_lr, test_set$income)
y_pred_lr = factor(ifelse(prob_pred > 0.5, ' >50K', ' <=50K'))
confusionMatrix(y_pred_lr, test_set$income)
# DECISION TREE
cv_dt = train(income ~ ., df, method="rpart",
control = rpart.control(cp=0.004),
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred_cv_dt = predict(cv_dt, newdata = test_set[-12], type = 'raw')
fancyRpartPlot(cv_dt$finalModel)
confusionMatrix(y_pred_cv_dt, test_set$income)
cv_dt
# LOGISTIC REGRESSION
cv_lr = train(income ~ ., method='glm', family = binomial, data = df,
trControl= trainControl(method="cv", number = 10, verboseIter = TRUE))
y_pred_cv_lr = predict(cv_lr, newdata = test_set[-12], type = 'raw')
confusionMatrix(y_pred_cv_lr, test_set$income)
cv_lr
lr
# T-TEST
results <- resamples(list(DT=cv_dt, LR=cv_lr))
summary(results)
plot(results)
summary(diff(results))
results$values
results$values$`DT~Accuracy`
results$values$`LR~Accuracy`
?t.test
DT = results$values$`DT~Accuracy`
LR = results$values$`LR~Accuracy`
t.test(DT,LR,paired=TRUE,alternative="greater")
t.test(DT,LR,paired=TRUE)
t.test(DT,LR,paired=TRUE,alternative="less")
t.test(DT,LR,paired=TRUE,alternative="greater")
t.test(DT,LR,paired=TRUE,alternative="less")
rnorm(10)
rnorm(10)
x = rnorm(10)
y = rnorm(10)
t.test(x,y)
t.test(DT,LR,paired=TRUE,alternative="less")
plot_coeffs_S <- function(mlr_model) {
coeffs <- sort(coefficients(mlr_model), decreasing = TRUE)  ### changed
mp <- barplot(coeffs, col="#3F97D0", xaxt='n', main="Regression Coefficients")
lablist <- names(coeffs)
text(mp, par("usr")[3], labels = lablist, srt = 45, adj = c(1.1,1.1), xpd = TRUE, cex=0.6)
}
plot_coeffs_S(cv_dt)
coeffs <- sort(coefficients(cv_lr), decreasing = TRUE)
cv_lr
coeffs <- sort(coefficients(lr), decreasing = TRUE)
plot_coeffs_S(lr)
coeffs
coeffs <- sort(abs(coefficients(lr)), decreasing = TRUE)
coeffs
#printcp(dt)
#plotcp(dt)
#summary(dt)
fancyRpartPlot(dt)
fancyRpartPlot(cv_dt$finalModel)
str(df)
